Throughout the course of these experiments, we have learned a number of lessons pertaining to the behavior of Spark for linear algebra computations in large-scale HPC systems. 
In this section, we share some of these lessons and conjecture on likely causes.

\paragraph{Spark Scheduling Bottlenecks.}
The Spark driver creates and sends tasks serially, which can cause bottlenecks at high concurrency.  This effect can be quantified by looking at two metrics: Task Start Delay and Scheduler Delay. Task Start Delay measures the time from the start of stage until the task is sent to an executor. Scheduler Delay measures the additional time until the driver receives confirmation that the task has been received and its execution has started. Figure~\ref{fig:hero-timeline} is a plot of a sample of the tasks from one stage of the 16TB Spark PCA run. Note that the ordering of the colored bars within each task line does not correspond to the order they occurred---Spark uses a pipelined execution model, where different portions of a task are interleaved at a fine grain, and reports the total time spent on each activity.  We can see that the scheduling bottleneck causes a uniform distribution of start times, with tasks starting as late as 20 seconds after the earliest task.  The scheduler delay grows with the start delay, indicating that confirmation messages are queuing up and waiting to be processed at the driver when it finishes sending new tasks.

\begin{table}[th]
\centering
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
Algo & Size & Nodes & Partitions & Time (s) & Measured & Predicted \\
{} & {} & {} & {} & {} & Task Start & Delay \\
{} & {} & {} & {} & {} & Delay (s) & (2000/sec) \\
\hline
\multirow{4}{*}{PCA} & \multirow{3}{*}{2.2 TB} & 100 & 3200 & 924 & 411 & 112 \\
 {} & {} & 300 & 9600 & 827 & 332 & 336 \\
 {} & {} & 500 & 16000 & 1160 & 542 & 560 \\ \cline{2-7} & & & & & & \\[-1ex]
 {} & {16 TB} & 1522 & 51200 & 3718 & 1779 & 1792 \\
 \hline
\end{tabular}
\caption{Spark scheduling delays.}
\label{tab:scheduling}
\end{table}

Ousterhout et al.~\cite{Ousterhout13Sparrow} showed that these factors limit the Spark scheduler to launching approximately 1500 tasks per second.  Their measurements were based on an older version of Spark from 2013, but there have been no significant changes to the scheduler. Our results on Cori are consistent with a similar rate of about 2000 tasks per second.  We show the impact of this bottleneck on PCA in Table~\ref{tab:scheduling}.  
We expect that the largest negative impact on scaling is caused by the wait required to schedule the tasks in each iteration. The \emph{Measured Task Start Delay} column shows the sum of the largest task start delays in each Spark stage.  The \emph{Predicted Delay} column shows the delay predicted by a scheduling rate of 2000 tasks per second over 70 iterations and the listed number of tasks/partitions.  We observe that at 300, 500, and 1522 nodes, the task start delay is very close to the predicted~value.

This bottleneck represents a limit on the scaling achievable by Spark for highly iterative algorithms.  In particular, as the amount of parallelism increases, the minimum number of partitions and tasks also increases.  This results in a linearly increasing overhead from the scheduler as we increase parallelism.  This delay is further multiplied by the number of iterations.  We see the impact of this in the PCA results in Table~\ref{tab:scheduling}, where the final column represents this fixed overhead and is thus a lower bound on how fast we can execute at the given scale.  

\begin{figure}[tbhp]
\centering
\includegraphics[width=.6\textwidth]{fig/spark_pca_hero_timeline.png}
\caption{A timeline of tasks on a particular node for a multiply Gramian stage during the 1522 node Spark run. }
\label{fig:hero-timeline}
\end{figure}

\begin{figure}[tbhp]
\centering
\includegraphics[width=.6\textwidth]{fig/pca_box_and_whiskers.png}
\caption{Distribution of various components of all tasks in a multiply Gramian stage in the Spark PCA hero run. }
\label{fig:whisker}
\end{figure}

\paragraph{Other Significant Spark Overheads.}
Figures~\ref{fig:nmfrt} and~\ref{fig:pcart} illustrate that a large block of time is spent in Task Overheads. These overheads consist of the shuffle read and write time, the task deserialization time (executor deserialize time), and result serialization time.  During our runs on Cori, most of these overheads are insignificant with the exception of the executor deserialize time, as can be seen in Figure~\ref{fig:whisker}. High executor deserialize times are usually attributable to large tasks that take a long time to unpack. Also, any time spent in garbage collection during the deserialize phase counts toward the deserialize time.

\paragraph{Spark Variability and Stragglers.}
The time waiting for stage to end bucket describes the idle time in which a task has finished, but the next stage has not started, so new tasks have not yet been scheduled. The main cause of this idle time is ``straggler" tasks, tasks that take a longer than average time to finish and thus hold up the next stage from starting. In Figure~\ref{fig:whisker}, we can see there is some variability in the multiply Gramian component of the tasks, but that this is insignificant compared to the Spark sheduler and task start delay, as well as the executor deserialization time. These overheads vary anywhere from less than a second to more than 20 seconds. This variation in task behavior leads to some task slots being left idle for up to 10 seconds.

The bulk-synchronous execution model of Spark creates scaling issues in the presence of stragglers. When a small number of tasks take much longer to complete, many cores waste cycles idling at synchronization barriers. At larger scales, we see increases in both the probability of at least one straggler, as well as the number of underutilized cores waiting at barriers.

During initial testing runs of the Spark PCA algorithm, variations in run time as large as 25\% were observed (in our staging runs we had a median run time of 645 seconds, a minium run time of 489 seconds, and a maximum run time of 716 seconds). These variations could not be attributed to any particular spark stage. Sometimes the delay would occur in the multiply Gramian step, other times in the intial data collect stage. This variability is illustrated in the box and whiskers plot. Spark has a ``speculation" functionality which aims to mitigate this variability by restarting straggling tasks on a new executor. However, we found that enabling speculation had no appreciable effect on improving the run time, because the overhead to fetch a portion of the RDD from another worker was sufficiently high. This is because requests for RDDs from other workers must wait until the worker finishes its running tasks. This can often result in delays that are as long as the run time of the straggling task.  

