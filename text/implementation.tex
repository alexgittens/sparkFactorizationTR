
\subsection{Spark as a Data Analytics Platform} 
%\textcolor{blue}

Spark is a parallel computing framework, built on the JVM, that adheres to the data parallelism model. A Spark cluster is composed of a driver process and a set of executor processes. The driver schedules and manages the work, which is carried out by the executors. The basic unit of work in Spark is called a task. A single executor has several slots for running tasks (by default, each core of an executor is mapped to one task) and runs several concurrent tasks in the course of calculations. Spark's primitive datatype is the resilient distributed dataset (RDD), a distributed array that is partitioned across the executors. The user-defined code that is to be run on the Spark cluster is called an application. When an application is submitted to the cluster, the driver analyses its computation graph and breaks it up into jobs.  Each job represents an action on the dataset, such as counting the number of entries, returning dataset entries, or saving a dataset to a file. Jobs are further broken down into stages, which are collections of tasks that execute the same code in parallel on a different subset of data. Each task operates on one partition of the RDD. Communication occurs only between stages, and takes the form of a shuffle, where all nodes communicate with each other, or a collect, where all nodes send data to the driver.

\subsection{Implementing Matrix Factorizations in Spark}

To match the data parallelism model of Spark, for all three algorithms the matrices are stored in a row-partitioned format.
Our algorithms were carefully chosen to take advantage of this format.  

The \textsc{MultiplyGramian} algorithm is the computational core of the PCA and CX algorithms.
This algorithm is applied efficiently in a distributed fashion by observing that if the $i$-th executor of $\ell$ stores the block of the rows of $A$ denoted by $A_{(i)},$ then $A^TA B = \sum_{i=1}^\ell A_{(i)}^T A_{(i)} B.$ Thus \textsc{MultiplyGramian} requires only one round of communication.  The local linear algebra primitives \textsc{QR} and \textsc{SVD} needed for PCA and CX are computed using the \textsc{LAPACK} bindings of the Breeze numerical linear algebra library.  The \textsc{Netlib-Java} binding of the \text{ARPACK} library supplies the \textsc{IRAM} primitive required by the PCA algorithm. 

The NMF algorithm has as its core the tall-skinny QR factorization, which is computed using a tree reduction over the row-block partitioned $A$.
We used the \textsc{TSQR} implementation available in the \textsc{Ml-Matrix} package. To implement the \textsc{XRay} algorithm, we use the \textsc{ML-Lib} non-negative least squares solver.


\subsection{Implementing Matrix Factorizations in C+MPI}
NMF, PCA and CX require linear algebra kernels that are widely available in libraries such as Intel MKL, Cray LibSci and arpack-ng. We make use of these libraries to perform linear algebra computations required for each matrix factorization. The data matrices are represented as 1D arrays of double-precision floating point numbers and are partitioned across multiple nodes. The matrix factorizations implemented and matrix shapes used require 1D-block partitioned layouts which enable us to use data parallel kernels such as matrix-vector products and TSQR. We predominately use MPI collectives for inter-processor communication and perform independent I/O using the Cray HDF5 parallel I/O~library.  