
\subsection{Spark as a Data Analytics Platform} \textcolor{blue}

Spark is a parallel computing framework, built on the JVM, that adheres to the data parallelism model. A Spark cluster is composed of a driver process and a set of executor processes. The driver controls and schedules the work, which is turn handed off to the exectors. The basic unit of work in Spark is called a task. A single executor has many slots for running tasks and will run several concurrent tasks in the course of calculations; although it can be changed, by default one task is assigned to one core available to the executor. Each user-defined code that is to be calculated is called an application. When an application is submitted to the cluster, the driver analyses it and breaks it up into jobs. Each job represents an action on the dataset, such as counting the number of entries, returning dataset extries, or saving a dataset to a file. Jobs are further broken down into stages, which are collections of tasks each executing the same code on a different subset of data. Spark converts input data into a resilient distributed dataset or RDD. Each RDD is split into multiple partitions and a single task works on a single partition.

\subsection{Implementing Matrix Factorizations in Spark}

\subsection{Implementing Matrix Factorizations in C+MPI}
NMF, PCA and CX require linear algebra kernels that are widely available in libraries such as Intel MKL, Cray LibSci and arpack-ng. We make use of these libraries to perform linear algebra computations required for each matrix factorization. The data matrices are represented as 1D arrays of double-precision floating point numbers and are partitioned across multiple nodes. The matrix factorizations implemented and matrix shapes used require 1D-block partitioned layouts which enable us to use data parallel kernels such as matrix-vector products and TSQR. We predominately use MPI collectives for inter-processor communication and perform independent I/O using the Cray HDF5 parallel I/O library.  