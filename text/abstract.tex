\begin{abstract}
%Data analysis has rapidly become an important part of scientific discovery. There exist many easy-to-use data analytics frameworks for domain scientists to use, however, they are well-suited for data-parallel tasks and record-based database-like transactions. In addition, scalability of these frameworks to large numbers of nodes has not been well-studied. 
We explore the trade-offs of performing linear algebra using Apache Spark on a high-performance platform and compare with traditional C and MPI implementations. Spark is designed for cluster computing platforms with access to local disks and is optimized for data-parallel and record-based, database-like tasks. We tune Spark to scale to large numbers of nodes on a Cray XC40 without local disks. We study three widely-used and important matrix factorizations (randomized PCA, randomized CX decompositions, and NMF) on Terabyte-scale data. PCA is ubiquitous; CX provides data interpretability; and NMF preserves physical plausibility. We perform these factorizations on data for which they are well-suited: PCA for climate modeling; CX for bioimaging; and NMF for particle physics. The data matrices are tall-and-skinny which enable the algorithms to map conveniently into Spark's data-parallel model. We perform scaling experiments, describe the sources of slowdowns, and provide tuning guidance to successfully attain peak performance using Spark.
\end{abstract}