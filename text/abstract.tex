\begin{abstract}
%Data analysis has rapidly become an important part of scientific discovery. There exist many easy-to-use data analytics frameworks for domain scientists to use, however, they are well-suited for data-parallel tasks and record-based database-like transactions. In addition, scalability of these frameworks to large numbers of nodes has not been well-studied. 
We explore the trade-offs of performing linear algebra using Apache Spark, compared to traditional C and MPI implementations on HPC platforms. Spark is designed for data analytics on cluster computing platforms with access to local disks and is optimized for data-parallel and database-like tasks. We examine three widely-used and important matrix factorizations: NMF (for physical plausability), PCA (for its ubiquity) and CX (for data interpretability). We apply these methods to TB-sized problems in particle physics, climate modeling and bioimaging.  The data matrices are tall-and-skinny which enable the algorithms to map conveniently into Spark's data-parallel model. We perform scaling experiments on up to 1600 Cray XC40 nodes, describe the sources of slowdowns, and provide tuning guidance to successfully attain peak performance using Spark. 
\end{abstract}