The extensive studies conducted in this paper lead us to the following conclusion: 
\begin{itemize}
\item{We have succeeded in implementing a range of important matrix factorization algorithms in Spark: we have applied NMF, PCA and CX to 1.6, 2.2, and 16-TB sized datasets. We have scaled the codes on 50, 100, 300, 500, and 1600 XC40 nodes. To the best of our knowledge, these are some of the largest scale \emph{scientific data analytics} workloads attempted with Spark on HPC systems.}
\item{Head-to-head comparisons of these methods with C+MPI variants have revealed a number of opportunities for improving Spark performance. The current end-to-end performance gap for our workloads is $2.3\times - 26\times$; if we remove I/O times the gap widens to $14.9\times - 43.8\times$. At scale, Spark performance overheads associated with scheduling, stragglers, result serialization and task deserialization completely dominate the runtime by an order of magnitude.}
\item{In order for Spark to leverage existing, high-performance linear algebra libraries, it may be worthwhile to investigate better mechanisms for integrating and interfacing MPI-based runtimes with Spark. The cost associated with copying data between the runtimes may not be prohibitive.}
\item{Finally, efficient, parallel I/O is critical for Data Analytics at scale. System architectures in the future will need to be balanced to support data-, and hence I/O-intensive, workloads.}
\end{itemize}